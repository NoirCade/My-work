네 안녕하세요 9팀의 팀장을 맡아 프로젝트를 진행한 김태호입니다
9팀에서는 '시각장애인을 위한 음료수 분류 AI 모델에 관한 연구'라는
이름의 프로젝트를 진행했습니다. 제목이 좀 거창한데요 ㅋㅋㅋ
왜 저희가 이 주제를 선정했는지부터 설명해드리면 이해가 가실것 같습니다

아마 미니 프로젝트 시작 첫 날에도 간단히 설명드려서 기억하시는 분들도
있을 것으로 생각되는데요, 처음 아이디어는 동하님이 제시해주셨습니다.
국내의 시각 장애인들 중 점자를 읽을 수 있는 분들의 비율은 겨우 12.4%로
약 8명 중 1명 정도만이 점자를 읽으실 수 있는 상황입니다.
다시말해 8명 중 7명은 점자로 적혀있더라도 읽으실 수 없다는 말이죠.
심지어 점자가 표기되어 있는 경우에도 문제가 있었습니다.
대부분의 음료에는 점자가 표기되어있긴 하지만, 그 내용은 아주 단순한
분류에 불과해 비장애인들처럼 충분한 선택의 기회를 가지지 못하는
문제가 있었습니다. 저희는 이것을 저희가 지금까지 배운 내용을 통해서
어느정도까지 해결해볼 수 있을지 시험해보고 싶었습니다.

그래서 해당 문제를 해결하기 위해 저희는 이런 기능을 가진
프로그램을 만들고자 했습니다.

시간적인 문제로 인해 사전에 선정된 6개의 음료에 대해서 분류하고자 했구요,
해당 음료들의 이미지를 학습한 모델을 웹캠에 적용하여 분류시키고자 했습니다.
마지막으로 시각적으로 인지하기 어려운 시각장애인들에게 도움을 주기위해
분류한 결과값을 TTS를 이용하여 소리로 출력할 수 있도록 했습니다.

진행 과정은 다음과 같습니다. 다른 팀들도 동일한 내용이니 간단히 보여드리고
넘어가겠습니다.  

그래서 먼저 1일차와 2일차 오전까지 데이터 수집 및 전처리 작업을 했습니다.
수집은 구글에서 이미지 크롤링을 진행했지만, 데이터량이 상당히 부족해서
웹캠과 핸드폰 캠을 이용하여 실물을 촬영해서 추가적으로 데이터를
확보했구요. 저희는 개별 음료 이미지만 찾아보다보니까 알지 못했는데, 
중간 발표 과정에서 타 팀에서 편의점 이미지 데이터를 사용한다는 이야기를
듣고 생각해보니 거기에 저희가 원하던 음료 이미지도 포함되어 있을 것
같아서 해당 자료도 받아서 추가했습니다.

크롤링을 통해서 라벨별로 100~300여개 데이터 수집했구요, 좌우반전과
로테이션 어그멘테이션으로 먼저 양을 좀 늘렸고, 해당 이미지의 선을 따서
배경 합성도 다양하게 진행하여 데이터를 많이 늘렸습니다.

그리고 이제 2일차 오후부터 3일차 오후까지 데이터셋을 구성하고 모델
학습을 진행했습니다. 편의점 이미지나 저희가 직접 핸드폰으로 촬영한
이미지는 용량이 상당히 커서 합성이나 어그멘테이션과 자료 공유에 자꾸
시간이 길어지기에 먼저 리사이징 해서 진행했구요
라벨별로 데이터 나눠줬습니다. 
아 그리고 이 페이지에 사실 진행 상황 중 발생한 이슈에 대한 비밀이
있는데요 ㅋㅋㅋ 뒤에가서 자세히 말씀드리겠습니다.

이후  3일차 저녁부터 4일차에는 각자 적용해보고 싶은 몇가지
모델들을 다양한 파라메터를 적용해서 실험해봤습니다.
batch size나 loss function, lr을 바꿔가면서 여러모로 살펴봤구요
모델은 저희가 vgg16, resnet18과 50, mobil-net v2와 v3, Alexnet 등
궁금했던 모델들을 이것저것 가져다 써봤구요
전체적으로 지표는 모든 모델에서 고르게 좋게 나왔지만, 실제 웹캠 적용시에
resnet 50이 가장 라벨을 잘 분류하는 것으로 보여 최종적으로는
resnet 50 모델을 선정했습니다.


4일차 오후에 들어서야 웹캠에 모델을 적용하고 실물을 보여주며 결과를
보았는데요. 아무래도 구성원들이 코드를 작성하고, 원하는 기능을 구현하는
데에 부족한 부분도 많다보니 그런 부분에도 시간이 오래걸렸지만 오류를
고치는 데에도 예상보다 많은 시간이 걸려 전체적으로 일정이 길어진 부분에서
아쉬움이 있었습니다.

아무튼, 웹캠에서 소리로 송출하기까지의 과정은 다음과 같습니다.

웹캠을 통해 이미지를 받아서 해당 음료가 어떤 음료인지, 그리고 그 확률이
어느정도인지를 표시하구요. 인식한 이미지가 맞을 확률이 85%이상일 경우,
해당 라벨값을 label.txt 파일로 저장하고, 그 텍스트 파일을 gTTS를
이용하여 mp3파일로 저장한 다음, playsound를 이용하여 즉시 소리로
이렇게 출력합니다.

사실 GUI까지 만들어보고 싶은 욕심도 있었지만, 시간적 한계로 해보지 못한
것도 조금 아쉬움이 남네요.

네 마지막으로 진행간에 있었던 주요 이슈 사항에 대해서 말씀드리겠습니다.
우선 테스트 진행시에 적중률이 굉장히 낮게 나오는 현상이 계속해서
발생하는 문제가 있었는데요. 처음에는 저희가 합성한 데이터가 너무 이질적이거나
데이터 량이 너무 적어서 제대로 학습되지 않아 이런 문제가 발생한다고 생각하여
아까 말씀드린 편의점 데이터 등을 추가적으로 넣고 학습을 진행해보았는데요,
그래도 지속적으로 같은 문제가 발생하여 단체로 한참을 헤맸던 문제였습니다. 
알고보니 데이터 스플릿 과정에서 라벨이 6개인데 테스트 데이터 폴더가 5개만...
생성되어서 그런 문제가 발생한 것이었습니다. 아까 중간에 말씀드린 이미지에도
폴더가 5개만 생성되어있었거든요.....ㅎ 그런 복선이 있었습니다.

다음은 웹캠에서의 인식률이 validation이나 test 결과에 비해 현저하게 낮게 나오는
이슈가 있었습니다. 저희는 이번에야말로 정말 데이터가 부족했는지, 아니면 학습했던
데이터에 비해서 조명이라던지, 배경이라던지 그런 캠을 이용한 환경이 너무 좋지 않아서
발생한 문제인지 파악하기가 어려웠습니다. 그래서 추가적인 어그멘테이션도 해보고,
혹시 또 코드의 문제인건 아닌지 찾아보다가 기존에 교육때 사용했던 웹캠 코드와
비교해보니 컴퓨터 화면에 출력되는 값은 채널을 변경해줬는데, 실제 테스트는 채널 변경
전에 입력된 값으로 진행되어 문제가 발생한 것으로 확인되었습니다.
채널 변경 부분을 수정하여 해결했습니다.

그밖에도 정말 사소한 것들로도 문제가 많았는데요. 해결법은 그리 어렵지 않은 단순한
문제들이 대부분이었지만 그런 사소한 것들 하나하나를 해결해가면서 파이썬과 AI개발에 대해서
정말 많이 배울 수 있었습니다.

이 프로젝트를 진행하면서 저희가 얻은 결론은 이렇습니다.
처음 목표했던대로 그동안 배운 내용들을 이용해서 실제로 모바일 환경에서 이용할 수 있는 단계는
아니지만, 시각장애인들에게 도움이 될 수 있는 프로그램을 만들어보았다는 점에 의의가 있었고,
코드를 통해서 기능들을 구현하고 그 과정에서 발생할 수 있는 다양한 상황들에 대해서 더 많이
경험해볼 필요를 느낄 수 있었습니다. 이상입니다